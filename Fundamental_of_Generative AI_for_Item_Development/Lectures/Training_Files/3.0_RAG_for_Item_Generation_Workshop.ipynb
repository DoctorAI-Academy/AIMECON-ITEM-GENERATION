{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5d17c43f",
      "metadata": {
        "id": "5d17c43f"
      },
      "source": [
        "# Retrieval-Augmented Generation (RAG) for Item Generation\n",
        "\n",
        "---\n",
        "\n",
        "## Workshop Overview\n",
        "\n",
        "Welcome to this hands-on workshop on using **Retrieval-Augmented Generation (RAG)** for item generation. This session will provide you with both theoretical understanding and practical implementation skills to build an AI-powered item generation tool that is grounded in authoritative content.\n",
        "\n",
        "### **What You'll Learn:**\n",
        "- The theoretical foundation of RAG and its applications in educational assessment\n",
        "- How to build a complete RAG pipeline using open-source tools\n",
        "- Best practices for generating high-quality, curriculum-aligned assessment items\n",
        "- Quality assurance and evaluation frameworks for AI-generated content\n",
        "- Ethical considerations and limitations in automated item generation\n",
        "\n",
        "### **Why RAG for Item Development?**\n",
        "Traditional AI language models can \"hallucinate\" or generate content that sounds plausible but isn't grounded in verified educational standards. RAG solves this by:\n",
        "\n",
        "**Retrieving** relevant content from authoritative sources (curriculum standards, textbooks, learning objectives)  \n",
        "**Augmenting** the language model with this context  \n",
        "**Generating** assessment items that are both creative and factually accurate\n",
        "\n",
        "Think of it as giving the AI a \"reference library\" before it writes your exam questions thereby ensuring every generated item is anchored to verified content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98096a8e",
        "outputId": "71e59a90-3c27-4ffe-bb18-a4babe2154f9"
      },
      "source": [
        "!pip install -U langchain-community langchain-text-splitters langchain-huggingface chromadb python-dotenv langchain-groq pypdf"
      ],
      "id": "98096a8e",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.3.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.3)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.3)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.38)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: groq<1.0.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.33.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.28.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.71.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INw-su8i0bNY",
        "outputId": "8671dad8-f5b3-4327-d2be-d5c0ea1f745a"
      },
      "id": "INw-su8i0bNY",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccec31ce",
      "metadata": {
        "id": "ccec31ce"
      },
      "source": [
        "## 1. Pipeline Overview\n",
        "\n",
        "A typical RAG pipeline consists of several stages.  Each stage plays a distinct role in making sure that the final generated item reflects accurate curriculum content and is easy for educators to trust:\n",
        "\n",
        "1. **Data ingestion and document preparation** ‚Äì gather curricular materials and convert them into a uniform format that the computer can process, e.g, PDFs.\n",
        "\n",
        "2. **Splitting the documents (‚Äúchunking‚Äù)** ‚Äì long texts are divided into smaller segments or *chunks*.  This is like breaking a textbook chapter into paragraphs so that the system can ‚Äúdigest‚Äù them.  Chunking is essential because language models can only process a limited amount of text at once; breaking the text into manageable pieces ensures that important details are not lost.\n",
        "\n",
        "3. **Embedding the documents** ‚Äì each chunk is transformed into a numerical vector that captures its meaning.  Embeddings are like fingerprints for text: they allow the computer to measure which passages are most similar to a given query.\n",
        "\n",
        "4. **Vector store indexing** ‚Äì all of these vectors are stored in a database designed to support similarity search.  You can think of it as a special index that lets you quickly find passages related to a topic.\n",
        "\n",
        "5. **Query and retrieval** ‚Äì when you have a question or item to generate, your query is also embedded and compared against the database to retrieve the most relevant chunks.\n",
        "\n",
        "6. **Generation with context** ‚Äì the retrieved text is combined with a large language model (LLM) to produce the assessment item.  Conditioning the model on actual curriculum content helps reduce hallucinations and ensures fidelity to the source material.\n",
        "\n",
        "7. **Evaluation and refinement** ‚Äì finally, review and refine the generated items.  Research shows that techniques like key‚Äëpoint extraction and careful prompting can improve coverage, grammar, and readability of items.\n",
        "\n",
        "In the following sections, we explore each stage in detail, with code examples using open‚Äësource models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78aab3a6",
      "metadata": {
        "id": "78aab3a6"
      },
      "source": [
        "## RAG Pipeline Visualization\n",
        "\n",
        "The following diagram illustrates the complete RAG pipeline workflow:\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"RAG_pipeline.png\" alt=\"RAG Pipeline Diagram\" width=\"700\" style=\"border: 1px solid #ddd; border-radius: 8px; padding: 10px;\">\n",
        "</div>\n",
        "\n",
        "*Figure 1: RAG Pipeline - From document ingestion to final item generation*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f38bc3",
      "metadata": {
        "id": "e9f38bc3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4bb50371",
      "metadata": {
        "id": "4bb50371"
      },
      "source": [
        "## 2. Stage¬†1 ‚Äì Data Ingestion and Preparation\n",
        "\n",
        "The first step is to collect the domain‚Äëspecific materials, which can be textbook chapters, lecture notes, curriculum standards, or any other documents that contain the knowledge your assessment should be based on.  These materials form the **knowledge base** that the RAG pipeline will consult.\n",
        "\n",
        "Once collected, we need to convert them into a format that LangChain can process.  This involves reading files from disk and, importantly, **splitting** long documents into smaller pieces.  Splitting (also called *chunking*) is necessary because both embedding models and generative models have a maximum context length.  By dividing a document into chunks, we ensure that each piece captures a coherent passage (for example, a paragraph or half‚Äëpage) and can be processed independently.  Later, when we search the knowledge base, we will be comparing these chunks for relevance.\n",
        "\n",
        "Below is an example using LangChain‚Äôs `DirectoryLoader` and `RecursiveCharacterTextSplitter` to load `.txt` files from a directory and split them into chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "93819561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93819561",
        "outputId": "42391ee3-fe55-4da9-c1a8-c683a2beef82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 468 documents and split into 901 chunks.\n",
            "\n",
            "Sample chunk from /content/gdrive/MyDrive/Fundamental_of_Generative AI_for_Item_Development/Lectures/Training_Files/data/Photosynthesis.pdf:\n",
            "Schematic of photosynthesis in plants. The carbohydrates\n",
            "produced are stored in or used by the plant.\n",
            "Composite image showing the global distribution of\n",
            "photosynthesis, including both oceanic phytopla...\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Path to your directory containing curricular PDF files.\n",
        "# Place PDF files in the data folder; each will be read as a separate document.\n",
        "DATA_DIR = \"/content/gdrive/MyDrive/Fundamental_of_Generative AI_for_Item_Development/Lectures/Training_Files/data\"  # Points to the data directory\n",
        "\n",
        "# Use DirectoryLoader to read PDF files into LangChain Document objects.\n",
        "# Each PDF becomes a Document with metadata about its source.\n",
        "directory_loader = DirectoryLoader(DATA_DIR, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
        "documents = directory_loader.load()\n",
        "\n",
        "# Split long documents into smaller chunks.  The chunk_size and chunk_overlap parameters\n",
        "# control the length of each chunk and how much neighbouring chunks overlap.  The overlap\n",
        "# helps preserve context across boundaries.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # number of characters per chunk\n",
        "    chunk_overlap=200 # overlapping characters between chunks to preserve context\n",
        ")\n",
        "\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "print(f\"Loaded {len(documents)} documents and split into {len(split_docs)} chunks.\")\n",
        "\n",
        "# Optional: Show a sample of what was loaded\n",
        "if split_docs:\n",
        "    print(f\"\\nSample chunk from {split_docs[0].metadata.get('source', 'unknown')}:\")\n",
        "    print(split_docs[0].page_content[:200] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520ab847",
      "metadata": {
        "id": "520ab847"
      },
      "source": [
        "## 3. Stage¬†2 ‚Äì Embedding Documents\n",
        "\n",
        "After splitting the documents, we translate each chunk into a numeric representation called an *embedding*.  An embedding model is a type of neural network that maps a sentence or paragraph to a high‚Äëdimensional vector such that semantically similar texts are close together in this space.  This translation step is crucial because it allows the computer to compare your query against thousands of document chunks quickly using simple mathematical operations.\n",
        "\n",
        "LangChain wraps many open‚Äësource embedding models from Hugging Face.  For instance, `sentence-transformers/all-MiniLM-L6-v2` is a lightweight model that produces 384‚Äëdimensional vectors well‚Äësuited for semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ef80aaf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef80aaf3",
        "outputId": "d2f17363-564f-48b4-f1a2-66b3f5fba405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed 901 embeddings using HuggingFace's all-MiniLM-L6-v2 model.\n",
            "Each embedding has 384 dimensions.\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Instantiate a HuggingFace embedding model. This model converts each chunk of text into\n",
        "# a high-dimensional vector (384 dimensions) that captures its semantic meaning.\n",
        "# The all-MiniLM-L6-v2 model is small, efficient, and runs locally - perfect for learning!\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'},  # Use CPU for compatibility\n",
        "    encode_kwargs={'normalize_embeddings': True}  # Normalize for better similarity search\n",
        ")\n",
        "\n",
        "# Compute embeddings for each split document. In a full application you would typically\n",
        "# pass the embedding model directly to the vector store without this intermediate step,\n",
        "# but computing them here demonstrates that each chunk is mapped to a numeric vector.\n",
        "embeddings = embedding_model.embed_documents([doc.page_content for doc in split_docs])\n",
        "print(f\"Computed {len(embeddings)} embeddings using HuggingFace's all-MiniLM-L6-v2 model.\")\n",
        "print(f\"Each embedding has {len(embeddings[0]) if embeddings else 0} dimensions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb42a64",
      "metadata": {
        "id": "4fb42a64"
      },
      "source": [
        "## 4. Stage 3 ‚Äì Building a Vector Store\n",
        "\n",
        "Once you have embeddings for all your document chunks, you need a way to organise and search them. A **vector store** is like a library catalogue for embeddings: it indexes each vector so that given a new query vector, it can quickly find the most similar ones.\n",
        "\n",
        "We'll use **ChromaDB**, an excellent open-source vector database that's easy to install and perfect for learning. ChromaDB automatically handles persistence, requires no complex setup, and provides fast similarity search. It stores each chunk's embedding together with its original text, making retrieval efficient and reliable.\n",
        "\n",
        "**Why ChromaDB?**\n",
        "- Simple installation: `pip install chromadb`\n",
        "- Automatic persistence to disk\n",
        "- No complex dependencies\n",
        "- Fast and reliable similarity search\n",
        "- Perfect for development and production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f76f61c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f76f61c2",
        "outputId": "d3fdfd92-4d7b-4379-94e2-c7e8758797a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating ChromaDB vector store...\n",
            "ChromaDB vector store created successfully!\n",
            "Stored in: ./chroma_db\n",
            "Indexed 901 document chunks\n",
            "Vector store persisted to disk for future use\n"
          ]
        }
      ],
      "source": [
        "# Using ChromaDB - A simple, fast vector database that's easy to install\n",
        "from langchain_community.vectorstores import Chroma\n",
        "import os\n",
        "\n",
        "# Create a unique directory for this session's vector store\n",
        "persist_directory = \"./chroma_db\"\n",
        "\n",
        "# Build a ChromaDB vector store directly from your split documents and embedding model\n",
        "print(\"Creating ChromaDB vector store...\")\n",
        "\n",
        "vector_store = Chroma.from_documents(\n",
        "    documents=split_docs,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "print(\"ChromaDB vector store created successfully!\")\n",
        "print(f\"Stored in: {persist_directory}\")\n",
        "print(f\"Indexed {len(split_docs)} document chunks\")\n",
        "print(f\"Vector store persisted to disk for future use\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fde475f9",
      "metadata": {
        "id": "fde475f9"
      },
      "source": [
        "## 5. Stage¬†4 ‚Äì Query and Retrieval\n",
        "\n",
        "To generate a new question, the user starts by formulating a **query**, which is a short prompt of the items to generate.  For example, ‚ÄúProvide 10 questions on linear algebra.‚Äù  This query is embedded using the same embedding model as before.  The vector store then finds the chunks whose embeddings are most similar to the query vector.  Retrieving these top‚Äë`k` chunks is similar to using a search engine: the model is effectively saying ‚Äúthese passages from the curriculum best answer your question.‚Äù  We will later feed these passages to the generative model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "920c5ba8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "920c5ba8",
        "outputId": "1bfb1e14-c46e-4564-fcc9-5e39c1d5ee3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading ChromaDB vector store from disk...\n",
            "‚úÖ ChromaDB vector store loaded successfully!\n",
            "üîç Searching for: 'Linear algebra concepts suitable for high school students'\n",
            "\n",
            "üìÑ Retrieved 3 relevant document chunks:\n",
            "======================================================================\n",
            "\n",
            "üìã Chunk 1 (Source: linearalgebra.pdf):\n",
            "--------------------------------------------------\n",
            "1\n",
            "What is Linear Algebra?\n",
            "Many diÔ¨Écult problems can be handled easily once relevant information is\n",
            "organized in a certain way. This text aims to teach you how to organize in-\n",
            "formation in cases where certain mathematical structures are present. Linear\n",
            "algebra is, in general, the study of those struc...\n",
            "--------------------------------------------------\n",
            "\n",
            "üìã Chunk 2 (Source: linearalgebra.pdf):\n",
            "--------------------------------------------------\n",
            "1\n",
            "What is Linear Algebra?\n",
            "Many diÔ¨Écult problems can be handled easily once relevant information is\n",
            "organized in a certain way. This text aims to teach you how to organize in-\n",
            "formation in cases where certain mathematical structures are present. Linear\n",
            "algebra is, in general, the study of those struc...\n",
            "--------------------------------------------------\n",
            "\n",
            "üìã Chunk 3 (Source: linearalgebra.pdf):\n",
            "--------------------------------------------------\n",
            "1\n",
            "What is Linear Algebra?\n",
            "Many diÔ¨Écult problems can be handled easily once relevant information is\n",
            "organized in a certain way. This text aims to teach you how to organize in-\n",
            "formation in cases where certain mathematical structures are present. Linear\n",
            "algebra is, in general, the study of those struc...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-846071817.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
            "  vector_store = Chroma(\n"
          ]
        }
      ],
      "source": [
        "# Load the ChromaDB vector store\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "persist_directory = \"./chroma_db\"\n",
        "\n",
        "# Load the existing ChromaDB vector store from disk\n",
        "print(\"Loading ChromaDB vector store from disk...\")\n",
        "\n",
        "vector_store = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "print(\"ChromaDB vector store loaded successfully!\")\n",
        "\n",
        "# Create a retriever from the vector store\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3}  # Return top 3 most similar chunks\n",
        ")\n",
        "\n",
        "# Example query: specify the concept you want to generate an item about\n",
        "query = \"Linear algebra concepts suitable for high school students\"\n",
        "print(f\"üîç Searching for: '{query}'\")\n",
        "\n",
        "retrieved_docs = retriever.invoke(query)\n",
        "\n",
        "print(f\"\\nüìÑ Retrieved {len(retrieved_docs)} relevant document chunks:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "    source = doc.metadata.get('source', 'unknown').split('/')[-1]  # Get filename only\n",
        "    print(f\"\\nüìã Chunk {i} (Source: {source}):\")\n",
        "    print(\"-\" * 50)\n",
        "    # Show first 300 characters for readability\n",
        "    content = doc.page_content.strip()\n",
        "    display_content = content[:300] + \"...\" if len(content) > 300 else content\n",
        "    print(display_content)\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcda139a",
      "metadata": {
        "id": "bcda139a"
      },
      "source": [
        "## 6. Stage¬†5 ‚Äì Generation with Context\n",
        "\n",
        "The heart of the RAG pipeline is the generation step.  Here we take the relevant passages retrieved in the previous stage and combine them with the query to form a prompt for a generative language model.  The model then produces a new assessment item (question and answer) that draws explicitly from the provided context.  This step reduces hallucination because the model is ‚Äúreminded‚Äù of the facts that should guide its answer.\n",
        "\n",
        "We use Groq-hosted open-source language models such as LLaMA 3 or Mixtral, which are fast, optimized, and freely accessible via the Groq API. In practice, you might choose a larger or more specialized model, but the overall code pattern remains the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4ae1969d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ae1969d",
        "outputId": "f04bd72e-da0b-4e0e-dae3-1cf989d57b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GROQ_API_KEY loaded successfully from Colab Secrets\n",
            "Generating 8th Grade Linear Algebra Questions\n",
            "============================================================\n",
            "\n",
            " Question Set 1:\n",
            "Query: Generate an 8th grade question about solving simple linear equations with one variable\n",
            "--------------------------------------------------\n",
            "Generated Question:\n",
            "Question: \n",
            "Let Œª represent the amount of sugar in each apple. If an apple has 3 units of sugar and an orange has twice as much sugar as an apple, how much sugar, s, is in a barrel that contains x apples and y oranges?\n",
            "\n",
            "A) s = 3x + 6y\n",
            "B) s = 3x - 6y\n",
            "C) s = 6x - 3y\n",
            "D) s = 3x + 3y\n",
            "\n",
            "Correct Answer: A\n",
            "Explanation:\n",
            "Step 1: We are given that an orange has twice as much sugar as an apple, and an apple has 3 units of sugar. This means an orange has 2 * 3 = 6 units of sugar.\n",
            "Step 2: In the barrel, there are x apples and y oranges. The total amount of sugar, s, in the barrel can be calculated by adding the sugar from the apples and the sugar from the oranges. The amount of sugar from the apples is 3x (since each apple has 3 units of sugar). The amount of sugar from the oranges is 6y (since each orange has 6 units of sugar).\n",
            "Step 3: To find the total amount of sugar, s, we add the sugar from the apples and the sugar from the oranges: s = 3x + 6y.\n",
            "\n",
            " Based on content from:\n",
            "   ‚Ä¢ linearalgebra.pdf: G\n",
            "Movie Scripts\n",
            "G.1 What is Linear Algebra?\n",
            "Hint for Review Problem 5\n",
            "Looking at the problem stateme...\n",
            "\n",
            "============================================================\n",
            "\n",
            " Question Set 2:\n",
            "Query: Create a basic algebra problem suitable for middle school students involving solving for x\n",
            "--------------------------------------------------\n",
            "Generated Question:\n",
            "Question: If Pablo has 5 grams of sugar from each apple, and oranges have twice as much sugar as apples, how many apples (x) will Pablo have if he has a total of 55 grams of sugar from both apples and oranges?\n",
            "\n",
            "A) x = 5\n",
            "B) x = 10\n",
            "C) x = 55 / 5 \n",
            "D) x = 55 / 2\n",
            "\n",
            "Correct Answer: B) x = 10\n",
            "Explanation: \n",
            "Let's say the number of apples is x. Since each apple has 5 grams of sugar, the total sugar from apples is 5x. The number of oranges is not given, but we know the total sugar is 55 grams. Since oranges have twice as much sugar as apples, the sugar from one orange is 2 * 5 = 10 grams. We can represent the total sugar as an equation:\n",
            "5x + 10y = 55\n",
            "Since the problem doesn't give us the number of oranges, we can't find y directly, but we know that x must be a whole number. Looking at the options, if x = 5, the total sugar from apples would be 5 * 5 = 25 grams, which is too little to reach 55 grams when combined with oranges. If x = 55 / 5, this would be the same as x = 11, but this is not among the options. But option B) x = 10 would result in a total sugar of 5 * 10 = 50 grams from the apples. To get the total sugar to 55 grams, we need 55 - 50 = 5 grams from the oranges. Since each orange has 10 grams of sugar, we would need 5 / 10 = 0.5 oranges, but since the number of oranges must be a whole number, this is not a valid solution. However, we can try option B) x = 10 to see if it works. If x = 10, the total sugar from apples is 5 * 10 = 50 grams. To get the total sugar to 55 grams, we need 55 - 50 = 5 grams from the oranges. Since each orange has 10 grams of sugar, we would need 5 / 10 = 0.5 oranges, but since the number of oranges must be a whole number, we can try a different number of oranges. If we let y = 1, the total sugar from oranges would be 10 * 1 = 10 grams. This is too little, so we need to try a larger number of oranges. If we let y = 2, the total sugar from oranges would be 10 * 2 = 20 grams. This is still too little, so we need to try an even larger number of oranges. If we let y = 3, the total sugar from oranges would be \n",
            "\n",
            " Based on content from:\n",
            "   ‚Ä¢ linearalgebra.pdf: 72 The Simplex Method\n",
            "Finally Pablo knows that oranges have twice as much sugar as apples and that a...\n",
            "\n",
            "============================================================\n",
            "\n",
            " Question Set 3:\n",
            "Query: Generate a linear equation problem that 8th graders can solve in 2-3 steps\n",
            "--------------------------------------------------\n",
            "Generated Question:\n",
            "Question: In Pablo's fruit problem, what is the minimum total number of apples and oranges that must be on the menu according to the teacher's and parent's fruit requirement?\n",
            "\n",
            "A) x + y ‚â• 10\n",
            "B) x + y ‚â• 15\n",
            "C) x + y ‚â• 20\n",
            "D) x + y ‚â• 30\n",
            "\n",
            "Correct Answer: B\n",
            "\n",
            "Explanation: \n",
            "\n",
            "In the given problem, we are asked to find the minimum total number of apples and oranges that must be on the menu according to the teacher's and parent's fruit requirement. This requirement is represented by the inequality x + y ‚â• 15, where x is the number of apples and y is the number of oranges.\n",
            "\n",
            "To solve this problem, we need to understand that the minimum total number of apples and oranges is the smallest value of x + y that satisfies the inequality x + y ‚â• 15. This is given directly by the inequality, and therefore the correct answer is x + y ‚â• 15.\n",
            "\n",
            " Based on content from:\n",
            "   ‚Ä¢ linearalgebra.pdf: 72 The Simplex Method\n",
            "Finally Pablo knows that oranges have twice as much sugar as apples and that a...\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from google.colab import userdata # Import userdata to access Colab Secrets\n",
        "\n",
        "\n",
        "groq_api_key = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "if not groq_api_key:\n",
        "    print(\"GROQ_API_KEY not found in Colab Secrets.\")\n",
        "    print(\"Please add your Groq API key to the Colab Secrets manager (key icon üîë) with the name 'GROQ_API_KEY'.\")\n",
        "else:\n",
        "    print(\"GROQ_API_KEY loaded successfully from Colab Secrets\")\n",
        "\n",
        "# Pick an OSS model served by Groq. Current supported models include:\n",
        "# - \"llama-3.1-8b-instant\" (recommended)\n",
        "# - \"llama-3.1-70b-versatile\"\n",
        "# - \"mixtral-8x7b-32768\"\n",
        "# - \"gemma2-9b-it\"\n",
        "llm = ChatGroq(\n",
        "    api_key=groq_api_key, # Pass the loaded API key\n",
        "    model=\"llama-3.1-8b-instant\",  # Updated to current supported model\n",
        "    temperature=0.6,     # lower = more deterministic\n",
        "    max_tokens=600\n",
        ")\n",
        "\n",
        "# 2) Prompt for 8th Grade Linear Algebra\n",
        "MATH_ITEM_PROMPT = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert 8th grade mathematics assessment writer specializing in linear algebra.\n",
        "Use ONLY the provided context to create ONE multiple-choice question suitable for 8th grade students.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Requirements for 8th Grade Linear Algebra:\n",
        "- Focus on basic linear equations (like ax + b = c or x + a = b)\n",
        "- Use simple integer solutions (avoid fractions when possible)\n",
        "- Create clear, direct question stems\n",
        "- EXACTLY 4 options labeled A‚ÄìD with ONE correct answer\n",
        "- Make distractors based on common student errors:\n",
        "  * Wrong operation (adding instead of subtracting)\n",
        "  * Wrong direction (subtracting from wrong side)\n",
        "  * Arithmetic errors\n",
        "  * Not performing the operation\n",
        "- Provide step-by-step explanation\n",
        "- Use variables like x, y, m, n (single letters)\n",
        "- Keep numbers simple (typically 1-50)\n",
        "\n",
        "Follow this exact format:\n",
        "Question: [Clear problem statement]\n",
        "A) [Correct answer]\n",
        "B) [Common error - wrong operation]\n",
        "C) [Common error - arithmetic mistake]\n",
        "D) [Common error - incomplete solution]\n",
        "Correct Answer: [Letter]\n",
        "Explanation: [Step-by-step solution showing the correct mathematical process]\n",
        "\n",
        "Examples of appropriate 8th grade topics from context:\n",
        "- Solving one-step equations: x + 5 = 12\n",
        "- Solving two-step equations: 2x + 3 = 11\n",
        "- Basic substitution problems\n",
        "- Simple linear relationships\n",
        "\n",
        "User goal: {question}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Helper function to format retrieved documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# 3) Build a RAG chain using LCEL for 8th grade math\n",
        "qa_chain = (\n",
        "    {\n",
        "        \"context\": retriever | format_docs,  # Uses ChromaDB retriever from Stage 4\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | MATH_ITEM_PROMPT\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# For getting source documents, create a separate chain\n",
        "\n",
        "qa_chain_with_sources = (\n",
        "    {\n",
        "        \"context\": retriever,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        ")\n",
        "\n",
        "# 4) Generate 8th grade linear algebra questions\n",
        "print(\"Generating 8th Grade Linear Algebra Questions\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test different types of 8th grade linear algebra problems\n",
        "grade_8_queries = [\n",
        "    \"Generate an 8th grade question about solving simple linear equations with one variable\",\n",
        "    \"Create a basic algebra problem suitable for middle school students involving solving for x\",\n",
        "    \"Generate a linear equation problem that 8th graders can solve in 2-3 steps\",\n",
        "]\n",
        "\n",
        "for i, query in enumerate(grade_8_queries, 1):\n",
        "    print(f\"\\n Question Set {i}:\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Get the response\n",
        "        response = qa_chain.invoke(query)\n",
        "        print(\"Generated Question:\")\n",
        "        print(response)\n",
        "\n",
        "        # Get source documents separately\n",
        "        source_docs = retriever.invoke(query)\n",
        "        print(f\"\\n Based on content from:\")\n",
        "        for j, doc in enumerate(source_docs[:1], 1):  # Show only first source\n",
        "            source = doc.metadata.get('source', 'unknown')\n",
        "            if '/' in source or '\\\\' in source:\n",
        "                source = source.split('\\\\')[-1].split('/')[-1]\n",
        "            print(f\"   ‚Ä¢ {source}: {doc.page_content[:100]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating question: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ef49062d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef49062d",
        "outputId": "1174a8d7-32e2-4ea6-bd27-7eea27615e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 901\n",
            "Retrieved 3 chunks for 'linear equations':\n",
            "1. /content/gdrive/MyDrive/Fundamental_of_Generative AI_for_Item_Development/Lectures/Training_Files/data/linearalgebra.pdf: 70 Systems of Linear Equations\n",
            "70...\n",
            "2. /content/gdrive/MyDrive/Fundamental_of_Generative AI_for_Item_Development/Lectures/Training_Files/data/linearalgebra.pdf: 70 Systems of Linear Equations\n",
            "70...\n"
          ]
        }
      ],
      "source": [
        "# Quick chunk check\n",
        "print(f\"Total chunks: {len(split_docs)}\")\n",
        "\n",
        "# Test one query\n",
        "docs = retriever.invoke(\"linear equations\")\n",
        "print(f\"Retrieved {len(docs)} chunks for 'linear equations':\")\n",
        "\n",
        "for i, doc in enumerate(docs[:2]):\n",
        "    source = doc.metadata.get('source', '').split('\\\\')[-1]\n",
        "    print(f\"{i+1}. {source}: {doc.page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "96ddff69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ddff69",
        "outputId": "895c69c7-3dfb-4acd-efd9-7d9bc6101a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä RAG Quality Assessment\n",
            "========================================\n",
            "Query: Generate a simple linear equation for 8th grade\n",
            "Generated Answer: Question: Solve for x in the equation x + 2 = 7.\n",
            "A) x = 9\n",
            "B) x = 5 + 2\n",
            "C) x = 5\n",
            "D) x = 9 + 2\n",
            "\n",
            "Correct Answer: C\n",
            "Explanation: To solve for x, we need to isolate x on one side of the equation. Since x i...\n",
            "Retrieved 3 source documents\n",
            "\n",
            "üìö Source Document Relevance Check:\n",
            "1. /content/gdrive/MyDrive/Fundamental_of_Generative AI_for_Item_Development/Lectures/Training_Files/data/linearalgebra.pdf: 70 Systems of Linear Equations\n",
            "70...\n",
            "2. /content/gdrive/MyDrive/Fundamental_of_Generative AI_for_Item_Development/Lectures/Training_Files/data/linearalgebra.pdf: 70 Systems of Linear Equations\n",
            "70...\n",
            "\n",
            "‚úÖ Manual Review Points:\n",
            "‚Ä¢ Does the generated question match the 8th grade level?\n",
            "‚Ä¢ Are the retrieved documents relevant to linear equations?\n",
            "‚Ä¢ Is the answer format appropriate for the context?\n",
            "‚Ä¢ Are the mathematical concepts accurate?\n"
          ]
        }
      ],
      "source": [
        "# Simple RAG Quality Check (Manual Evaluation)\n",
        "# Since RAGAS setup can be complex, let's do a simple manual evaluation\n",
        "\n",
        "print(\"RAG Quality Assessment\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Test question generation\n",
        "test_query = \"Generate a simple linear equation for 8th grade\"\n",
        "response = qa_chain.invoke(test_query)\n",
        "\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"Generated Answer: {response[:200]}...\")\n",
        "\n",
        "# Retrieve source documents separately\n",
        "source_documents = retriever.invoke(test_query)\n",
        "print(f\"Retrieved {len(source_documents)} source documents\")\n",
        "\n",
        "# Show relevance of retrieved documents\n",
        "print(\"\\nSource Document Relevance Check:\")\n",
        "for i, doc in enumerate(source_documents[:2], 1):\n",
        "    source = doc.metadata.get('source', '').split('\\\\')[-1]\n",
        "    content = doc.page_content[:150]\n",
        "    print(f\"{i}. {source}: {content}...\")\n",
        "\n",
        "print(\"\\nManual Review Points:\")\n",
        "print(\"‚Ä¢ Does the generated question match the 8th grade level?\")\n",
        "print(\"‚Ä¢ Are the retrieved documents relevant to linear equations?\")\n",
        "print(\"‚Ä¢ Is the answer format appropriate for the context?\")\n",
        "print(\"‚Ä¢ Are the mathematical concepts accurate?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c85d074",
      "metadata": {
        "id": "0c85d074"
      },
      "source": [
        "## 7. Stage¬†6 ‚Äì Evaluation and Refinement\n",
        "\n",
        "Automatically generated questions should not be used blindly; users need to review and refine them.  Research in retrieval‚Äëaugmented item generation has shown that methods like key‚Äëpoint extraction and careful prompting can improve vital coverage, grammar and readability.  Some practical evaluation strategies include:\n",
        "\n",
        "- **Content alignment** ‚Äì verify that each generated item accurately assesses the intended concept and at the appropriate cognitive level (e.g., recall, application, analysis).\n",
        "- **Correctness and clarity** ‚Äì check that the question is unambiguous and that the answer provided is correct.\n",
        "- **Difficulty and distractor quality** ‚Äì adjust the difficulty of multiple‚Äëchoice questions and ensure distractors (incorrect options) are plausible but clearly wrong.\n",
        "\n",
        "For automation, **RAGAS** (Retrieval-Augmented Generation Assessment Suite) offers useful metrics:  \n",
        "- **Context relevance** ‚Äì retrieved passages match the query.  \n",
        "- **Faithfulness** ‚Äì generation stays true to the context.  \n",
        "- **Answer correctness** ‚Äì answer is supported by evidence.  \n",
        "\n",
        "Combining quick RAGAS diagnostics with **SME feedback** creates an efficient refinement loop, leading to higher-quality, trustworthy items.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5b730b",
      "metadata": {
        "id": "ea5b730b"
      },
      "source": [
        "## 8. Conclusion\n",
        "\n",
        "By following these stages, collecting and splitting your knowledge base, embedding it, indexing it in a vector store, retrieving relevant passages, generating with context, and evaluating the results, you can build a retrieval‚Äëaugmented item generator tailored to your domain.  RAG‚Äôs strength lies in anchoring generative models to external knowledge, thereby producing responses that are both relevant and factual.  The LangChain framework provides convenient abstractions for each stage, and open‚Äësource models make it accessible to everyone without proprietary licenses.  Adapt the code provided to your own knowledge base and continue experimenting with different models and prompts to achieve the best results."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}