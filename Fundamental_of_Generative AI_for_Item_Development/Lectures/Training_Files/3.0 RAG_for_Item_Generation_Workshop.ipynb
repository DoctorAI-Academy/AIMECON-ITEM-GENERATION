{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d17c43f",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) for Item Generation\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Overview\n",
    "\n",
    "Welcome to this hands-on workshop on using **Retrieval-Augmented Generation (RAG)** for item generation. This session will provide you with both theoretical understanding and practical implementation skills to build an AI-powered item generation tool that is grounded in authoritative content.\n",
    "\n",
    "### **What You'll Learn:**\n",
    "- The theoretical foundation of RAG and its applications in educational assessment\n",
    "- How to build a complete RAG pipeline using open-source tools\n",
    "- Best practices for generating high-quality, curriculum-aligned assessment items\n",
    "- Quality assurance and evaluation frameworks for AI-generated content\n",
    "- Ethical considerations and limitations in automated item generation\n",
    "\n",
    "### **Why RAG for Item Development?**\n",
    "Traditional AI language models can \"hallucinate\" or generate content that sounds plausible but isn't grounded in verified educational standards. RAG solves this by:\n",
    "\n",
    "**Retrieving** relevant content from authoritative sources (curriculum standards, textbooks, learning objectives)  \n",
    "**Augmenting** the language model with this context  \n",
    "**Generating** assessment items that are both creative and factually accurate\n",
    "\n",
    "Think of it as giving the AI a \"reference library\" before it writes your exam questions thereby ensuring every generated item is anchored to verified content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec31ce",
   "metadata": {},
   "source": [
    "## 1. Pipeline Overview\n",
    "\n",
    "A typical RAG pipeline consists of several stages.  Each stage plays a distinct role in making sure that the final generated item reflects accurate curriculum content and is easy for educators to trust:\n",
    "\n",
    "1. **Data ingestion and document preparation** ‚Äì gather curricular materials and convert them into a uniform format that the computer can process, e.g, PDFs.\n",
    "\n",
    "2. **Splitting the documents (‚Äúchunking‚Äù)** ‚Äì long texts are divided into smaller segments or *chunks*.  This is like breaking a textbook chapter into paragraphs so that the system can ‚Äúdigest‚Äù them.  Chunking is essential because language models can only process a limited amount of text at once; breaking the text into manageable pieces ensures that important details are not lost.\n",
    "\n",
    "3. **Embedding the documents** ‚Äì each chunk is transformed into a numerical vector that captures its meaning.  Embeddings are like fingerprints for text: they allow the computer to measure which passages are most similar to a given query.\n",
    "\n",
    "4. **Vector store indexing** ‚Äì all of these vectors are stored in a database designed to support similarity search.  You can think of it as a special index that lets you quickly find passages related to a topic.\n",
    "\n",
    "5. **Query and retrieval** ‚Äì when you have a question or item to generate, your query is also embedded and compared against the database to retrieve the most relevant chunks.\n",
    "\n",
    "6. **Generation with context** ‚Äì the retrieved text is combined with a large language model (LLM) to produce the assessment item.  Conditioning the model on actual curriculum content helps reduce hallucinations and ensures fidelity to the source material.\n",
    "\n",
    "7. **Evaluation and refinement** ‚Äì finally, review and refine the generated items.  Research shows that techniques like key‚Äëpoint extraction and careful prompting can improve coverage, grammar, and readability of items.\n",
    "\n",
    "In the following sections, we explore each stage in detail, with code examples using open‚Äësource models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aab3a6",
   "metadata": {},
   "source": [
    "## RAG Pipeline Visualization\n",
    "\n",
    "The following diagram illustrates the complete RAG pipeline workflow:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"RAG_pipeline.png\" alt=\"RAG Pipeline Diagram\" width=\"700\" style=\"border: 1px solid #ddd; border-radius: 8px; padding: 10px;\">\n",
    "</div>\n",
    "\n",
    "*Figure 1: RAG Pipeline - From document ingestion to final item generation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f38bc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bb50371",
   "metadata": {},
   "source": [
    "## 2. Stage¬†1 ‚Äì Data Ingestion and Preparation\n",
    "\n",
    "The first step is to collect the domain‚Äëspecific materials, which can be textbook chapters, lecture notes, curriculum standards, or any other documents that contain the knowledge your assessment should be based on.  These materials form the **knowledge base** that the RAG pipeline will consult.\n",
    "\n",
    "Once collected, we need to convert them into a format that LangChain can process.  This involves reading files from disk and, importantly, **splitting** long documents into smaller pieces.  Splitting (also called *chunking*) is necessary because both embedding models and generative models have a maximum context length.  By dividing a document into chunks, we ensure that each piece captures a coherent passage (for example, a paragraph or half‚Äëpage) and can be processed independently.  Later, when we search the knowledge base, we will be comparing these chunks for relevance.\n",
    "\n",
    "Below is an example using LangChain‚Äôs `DirectoryLoader` and `RecursiveCharacterTextSplitter` to load `.txt` files from a directory and split them into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93819561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 468 documents and split into 901 chunks.\n",
      "\n",
      "Sample chunk from data\\linearalgebra.pdf:\n",
      "Linear Algebra\n",
      "David Cherney, Tom Denton,\n",
      "Rohit Thomas and Andrew Waldron...\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Path to your directory containing curricular PDF files.\n",
    "# Place PDF files in the data folder; each will be read as a separate document.\n",
    "DATA_DIR = \"./data\"  # Points to the data directory\n",
    "\n",
    "# Use DirectoryLoader to read PDF files into LangChain Document objects.\n",
    "# Each PDF becomes a Document with metadata about its source.\n",
    "directory_loader = DirectoryLoader(DATA_DIR, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = directory_loader.load()\n",
    "\n",
    "# Split long documents into smaller chunks.  The chunk_size and chunk_overlap parameters\n",
    "# control the length of each chunk and how much neighbouring chunks overlap.  The overlap\n",
    "# helps preserve context across boundaries.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # number of characters per chunk\n",
    "    chunk_overlap=200 # overlapping characters between chunks to preserve context\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "print(f\"Loaded {len(documents)} documents and split into {len(split_docs)} chunks.\")\n",
    "\n",
    "# Optional: Show a sample of what was loaded\n",
    "if split_docs:\n",
    "    print(f\"\\nSample chunk from {split_docs[0].metadata.get('source', 'unknown')}:\")\n",
    "    print(split_docs[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ab847",
   "metadata": {},
   "source": [
    "## 3. Stage¬†2 ‚Äì Embedding Documents\n",
    "\n",
    "After splitting the documents, we translate each chunk into a numeric representation called an *embedding*.  An embedding model is a type of neural network that maps a sentence or paragraph to a high‚Äëdimensional vector such that semantically similar texts are close together in this space.  This translation step is crucial because it allows the computer to compare your query against thousands of document chunks quickly using simple mathematical operations.\n",
    "\n",
    "LangChain wraps many open‚Äësource embedding models from Hugging Face.  For instance, `sentence-transformers/all-MiniLM-L6-v2` is a lightweight model that produces 384‚Äëdimensional vectors well‚Äësuited for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef80aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed 901 embeddings using HuggingFace's all-MiniLM-L6-v2 model.\n",
      "Each embedding has 384 dimensions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Instantiate a HuggingFace embedding model. This model converts each chunk of text into\n",
    "# a high-dimensional vector (384 dimensions) that captures its semantic meaning.\n",
    "# The all-MiniLM-L6-v2 model is small, efficient, and runs locally - perfect for learning!\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use CPU for compatibility\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Normalize for better similarity search\n",
    ")\n",
    "\n",
    "# Compute embeddings for each split document. In a full application you would typically\n",
    "# pass the embedding model directly to the vector store without this intermediate step,\n",
    "# but computing them here demonstrates that each chunk is mapped to a numeric vector.\n",
    "embeddings = embedding_model.embed_documents([doc.page_content for doc in split_docs])\n",
    "print(f\"Computed {len(embeddings)} embeddings using HuggingFace's all-MiniLM-L6-v2 model.\")\n",
    "print(f\"Each embedding has {len(embeddings[0]) if embeddings else 0} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb42a64",
   "metadata": {},
   "source": [
    "## 4. Stage 3 ‚Äì Building a Vector Store\n",
    "\n",
    "Once you have embeddings for all your document chunks, you need a way to organise and search them. A **vector store** is like a library catalogue for embeddings: it indexes each vector so that given a new query vector, it can quickly find the most similar ones. \n",
    "\n",
    "We'll use **ChromaDB**, an excellent open-source vector database that's easy to install and perfect for learning. ChromaDB automatically handles persistence, requires no complex setup, and provides fast similarity search. It stores each chunk's embedding together with its original text, making retrieval efficient and reliable.\n",
    "\n",
    "**Why ChromaDB?**\n",
    "- Simple installation: `pip install chromadb`\n",
    "- Automatic persistence to disk\n",
    "- No complex dependencies \n",
    "- Fast and reliable similarity search\n",
    "- Perfect for development and production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f76f61c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating ChromaDB vector store...\n",
      "‚úÖ ChromaDB vector store created successfully!\n",
      "üìÅ Stored in: ./chroma_db\n",
      "üìä Indexed 901 document chunks\n",
      "üíæ Vector store persisted to disk for future use\n"
     ]
    }
   ],
   "source": [
    "# Using ChromaDB - A simple, fast vector database that's easy to install\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "# Create a unique directory for this session's vector store\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Build a ChromaDB vector store directly from your split documents and embedding model\n",
    "print(\"üöÄ Creating ChromaDB vector store...\")\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ChromaDB vector store created successfully!\")\n",
    "print(f\"üìÅ Stored in: {persist_directory}\")\n",
    "print(f\"üìä Indexed {len(split_docs)} document chunks\")\n",
    "print(f\"üíæ Vector store persisted to disk for future use\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde475f9",
   "metadata": {},
   "source": [
    "## 5. Stage¬†4 ‚Äì Query and Retrieval\n",
    "\n",
    "To generate a new question, the user starts by formulating a **query**, which is a short prompt of the items to generate.  For example, ‚ÄúProvide 10 questions on linear algebra.‚Äù  This query is embedded using the same embedding model as before.  The vector store then finds the chunks whose embeddings are most similar to the query vector.  Retrieving these top‚Äë`k` chunks is similar to using a search engine: the model is effectively saying ‚Äúthese passages from the curriculum best answer your question.‚Äù  We will later feed these passages to the generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "920c5ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading ChromaDB vector store from disk...\n",
      "‚úÖ ChromaDB vector store loaded successfully!\n",
      "üîç Searching for: 'Linear algebra concepts suitable for high school students'\n",
      "\n",
      "üìÑ Retrieved 3 relevant document chunks:\n",
      "======================================================================\n",
      "\n",
      "üìã Chunk 1 (Source: data\\linearalgebra.pdf):\n",
      "--------------------------------------------------\n",
      "1\n",
      "What is Linear Algebra?\n",
      "Many diÔ¨Écult problems can be handled easily once relevant information is\n",
      "organized in a certain way. This text aims to teach you how to organize in-\n",
      "formation in cases where certain mathematical structures are present. Linear\n",
      "algebra is, in general, the study of those struc...\n",
      "--------------------------------------------------\n",
      "\n",
      "üìã Chunk 2 (Source: data\\linearalgebra.pdf):\n",
      "--------------------------------------------------\n",
      "12 What is Linear Algebra?\n",
      "This example is a hint at a much bigger idea central to the text; our choice of\n",
      "order is an example of choosing a basis3.\n",
      "The main lesson of an introductory linear algebra course is this: you\n",
      "have considerable freedom in how you organize information about certain\n",
      "functions...\n",
      "--------------------------------------------------\n",
      "\n",
      "üìã Chunk 3 (Source: data\\linearalgebra.pdf):\n",
      "--------------------------------------------------\n",
      "Linear Algebra\n",
      "David Cherney, Tom Denton,\n",
      "Rohit Thomas and Andrew Waldron\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the ChromaDB vector store \n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Load the existing ChromaDB vector store from disk\n",
    "print(\"üîÑ Loading ChromaDB vector store from disk...\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ChromaDB vector store loaded successfully!\")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\": 3}  # Return top 3 most similar chunks\n",
    ")\n",
    "\n",
    "# Example query: specify the concept you want to generate an item about\n",
    "query = \"Linear algebra concepts suitable for high school students\"\n",
    "docs = retriever.invoke(query) \n",
    "print(f\"üîç Searching for: '{query}'\")\n",
    "\n",
    "\n",
    "print(f\"\\nüìÑ Retrieved {len(docs)} relevant document chunks:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    source = doc.metadata.get('source', 'unknown').split('/')[-1]  # Get filename only\n",
    "    print(f\"\\nüìã Chunk {i} (Source: {source}):\")\n",
    "    print(\"-\" * 50)\n",
    "    # Show first 300 characters for readability\n",
    "    content = doc.page_content.strip()\n",
    "    display_content = content[:300] + \"...\" if len(content) > 300 else content\n",
    "    print(display_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda139a",
   "metadata": {},
   "source": [
    "## 6. Stage¬†5 ‚Äì Generation with Context\n",
    "\n",
    "The heart of the RAG pipeline is the generation step.  Here we take the relevant passages retrieved in the previous stage and combine them with the query to form a prompt for a generative language model.  The model then produces a new assessment item (question and answer) that draws explicitly from the provided context.  This step reduces hallucination because the model is ‚Äúreminded‚Äù of the facts that should guide its answer.\n",
    "\n",
    "We use Groq-hosted open-source language models such as LLaMA 3 or Mixtral, which are fast, optimized, and freely accessible via the Groq API. In practice, you might choose a larger or more specialized model, but the overall code pattern remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ae1969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 8th Grade Linear Algebra Questions\n",
      "============================================================\n",
      "\n",
      "üî¢ Question Set 1:\n",
      "Query: Generate an 8th grade question about solving simple linear equations with one variable\n",
      "--------------------------------------------------\n",
      "Generated Question:\n",
      "Question: \n",
      "\n",
      "Maria has 3 apples and 6 bananas in her bag. She wants to know how many pieces of fruit are in her bag in total. If x represents the number of apples and y represents the number of bananas, and f represents the total number of pieces of fruit, write an equation in terms of x and y that represents the situation. \n",
      "\n",
      "A) f = x + 2y\n",
      "B) f = y - x\n",
      "C) f = 2x + 3y\n",
      "D) f = x - 2y\n",
      "\n",
      "Correct Answer: A) f = x + y\n",
      "\n",
      "Explanation: \n",
      "\n",
      "Step 1: Identify the relationship between the variables.\n",
      "We are told that x represents the number of apples, y represents the number of bananas, and f represents the total number of pieces of fruit.\n",
      "\n",
      "Step 2: Determine the equation that represents the situation.\n",
      "Since each piece of fruit is either an apple or a banana, the total number of pieces of fruit (f) is the sum of the number of apples (x) and the number of bananas (y). \n",
      "\n",
      "Step 3: Write the equation in terms of x and y.\n",
      "f = x + y\n",
      "\n",
      "The correct answer is A) f = x + y.\n",
      "\n",
      "üìö Based on content from:\n",
      "   ‚Ä¢ linearalgebra.pdf: G\n",
      "Movie Scripts\n",
      "G.1 What is Linear Algebra?\n",
      "Hint for Review Problem 5\n",
      "Looking at the problem stateme...\n",
      "\n",
      "============================================================\n",
      "\n",
      "üî¢ Question Set 2:\n",
      "Query: Create a basic algebra problem suitable for middle school students involving solving for x\n",
      "--------------------------------------------------\n",
      "Generated Question:\n",
      "Question: Tom has 3 more apples than bananas. If he has a total of 7 fruits, how many apples (x) does Tom have?\n",
      "\n",
      "A) x = 4\n",
      "B) x = 5 + 3 \n",
      "C) x = 7 - 3\n",
      "D) x = 7\n",
      "\n",
      "Correct Answer: A) x = 4\n",
      "Explanation: Let's use a variable to represent the number of bananas, y. We know that Tom has 3 more apples than bananas, so the number of apples (x) can be expressed as x = y + 3. We are also given that the total number of fruits (apples + bananas) is 7, so we can write the equation x + y = 7. Substituting x = y + 3 into the equation, we get (y + 3) + y = 7. Simplifying, we get 2y + 3 = 7. Subtracting 3 from both sides, we get 2y = 4. Dividing both sides by 2, we get y = 2. Now that we know y = 2, we can substitute this value back into the equation x = y + 3 to find x. x = 2 + 3, so x = 5. However, this is not the correct answer because it says \"Tom has 3 more apples than bananas.\" We know y = 2 and x = 5, which means Tom has 3 more bananas than apples. This is the opposite of the statement in the problem. Therefore, we know that y = 3 and x = 6 is not the correct answer. However, we know that x = 3 + 3 is not the answer because it says \"If he has a total of 7 fruits.\" We know x + y = 7 and y = 3, so x = 4.\n",
      "\n",
      "üìö Based on content from:\n",
      "   ‚Ä¢ linearalgebra.pdf: 72 The Simplex Method\n",
      "Finally Pablo knows that oranges have twice as much sugar as apples and that a...\n",
      "\n",
      "============================================================\n",
      "\n",
      "üî¢ Question Set 3:\n",
      "Query: Generate a linear equation problem that 8th graders can solve in 2-3 steps\n",
      "--------------------------------------------------\n",
      "Generated Question:\n",
      "Question: \n",
      "Tom has 5 apples and 7 oranges. If he wants to add more apples and oranges to keep the total sugar content as low as possible, and he knows that the total number of fruits must be at least 15 but no more than 25, what is the number of apples he should add?\n",
      "\n",
      "A) x + 5 = 12\n",
      "B) x + 5 = 20\n",
      "C) x + 5 = 22\n",
      "D) x + 5 = 25\n",
      "\n",
      "Correct Answer: B\n",
      "Explanation: \n",
      "Given that Tom already has 5 apples and 7 oranges, the total number of fruits he has is 5 + 7 = 12. To keep the total number of fruits at least 15 but no more than 25, we can write the equation 12 + x ‚â• 15 and 12 + x ‚â§ 25. Since we want the minimum number of fruits, we can set up the equation 12 + x = 15 and solve for x.\n",
      "\n",
      "First, subtract 12 from both sides of the equation: \n",
      "12 + x - 12 = 15 - 12 \n",
      "x = 3 \n",
      "\n",
      "However, this is not among the options. To find the correct option, let's check the other options:\n",
      "Option A: x + 5 = 12 => x = 7, which is more than the minimum number of fruits.\n",
      "Option C: x + 5 = 22 => x = 17, which is more than the minimum number of fruits.\n",
      "Option D: x + 5 = 25 => x = 20, which is more than the minimum number of fruits.\n",
      "The correct option among the ones given is option B, which is x + 5 = 20, but it should be x = 15, the correct solution is not among the options, however, option B is the closest to the minimum number of fruits.\n",
      "\n",
      "üìö Based on content from:\n",
      "   ‚Ä¢ linearalgebra.pdf: 72 The Simplex Method\n",
      "Finally Pablo knows that oranges have twice as much sugar as apples and that a...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Install the Groq LangChain integration if needed:\n",
    "# !pip install -U langchain_groq\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate         # <-- import from core\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain  \n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# 1) Configure the Groq model (uses open-source OSS models hosted by Groq)\n",
    "# The API key is now loaded from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    raise RuntimeError(\"GROQ_API_KEY missing in .env\")\n",
    "\n",
    "# Pick an OSS model served by Groq. Current supported models include:\n",
    "# - \"llama-3.1-8b-instant\" (recommended)\n",
    "# - \"llama-3.1-70b-versatile\"\n",
    "# - \"mixtral-8x7b-32768\"\n",
    "# - \"gemma2-9b-it\"\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  # Updated to current supported model\n",
    "    temperature=0.6,     # lower = more deterministic\n",
    "    max_tokens=600\n",
    ")\n",
    "\n",
    "# 2) Prompt for 8th Grade Linear Algebra\n",
    "MATH_ITEM_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert 8th grade mathematics assessment writer specializing in linear algebra.\n",
    "Use ONLY the provided context to create ONE multiple-choice question suitable for 8th grade students.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Requirements for 8th Grade Linear Algebra:\n",
    "- Focus on basic linear equations (like ax + b = c or x + a = b)\n",
    "- Use simple integer solutions (avoid fractions when possible)\n",
    "- Create clear, direct question stems\n",
    "- EXACTLY 4 options labeled A‚ÄìD with ONE correct answer\n",
    "- Make distractors based on common student errors:\n",
    "  * Wrong operation (adding instead of subtracting)\n",
    "  * Wrong direction (subtracting from wrong side)\n",
    "  * Arithmetic errors\n",
    "  * Not performing the operation\n",
    "- Provide step-by-step explanation\n",
    "- Use variables like x, y, m, n (single letters)\n",
    "- Keep numbers simple (typically 1-50)\n",
    "\n",
    "Follow this exact format:\n",
    "Question: [Clear problem statement]\n",
    "A) [Correct answer]\n",
    "B) [Common error - wrong operation]\n",
    "C) [Common error - arithmetic mistake]  \n",
    "D) [Common error - incomplete solution]\n",
    "Correct Answer: [Letter]\n",
    "Explanation: [Step-by-step solution showing the correct mathematical process]\n",
    "\n",
    "Examples of appropriate 8th grade topics from context:\n",
    "- Solving one-step equations: x + 5 = 12\n",
    "- Solving two-step equations: 2x + 3 = 11\n",
    "- Basic substitution problems\n",
    "- Simple linear relationships\n",
    "\n",
    "User goal: {input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 3) Build a RetrievalQA chain for 8th grade math\n",
    "# Build a document chain (LLM + prompt expects `context`)\n",
    "doc_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=MATH_ITEM_PROMPT,\n",
    "    document_variable_name=\"context\",   # maps the docs' combined text into {context}\n",
    ")\n",
    "\n",
    "\n",
    "# Build the retrieval chain (retriever -> doc_chain)\n",
    "qa_chain = create_retrieval_chain(\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=doc_chain,\n",
    ")\n",
    "\n",
    "print(\"Generating 8th Grade Linear Algebra Questions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different types of 8th grade linear algebra problems\n",
    "grade_8_queries = [\n",
    "    \"Generate an 8th grade question about solving simple linear equations with one variable\",\n",
    "    \"Create a basic algebra problem suitable for middle school students involving solving for x\",\n",
    "    \"Generate a linear equation problem that 8th graders can solve in 2-3 steps\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(grade_8_queries, 1):\n",
    "    print(f\"\\nüî¢ Question Set {i}:\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # ‚úÖ Runnable API\n",
    "        out = qa_chain.invoke({\"input\": query})\n",
    "\n",
    "        # In v0.3, create_retrieval_chain typically returns:\n",
    "        #   {\"answer\": <str>, \"context\": [Document, ...]}\n",
    "        answer_text = out.get(\"answer\") or out.get(\"output_text\") or str(out)\n",
    "        docs = out.get(\"context\", [])\n",
    "\n",
    "        print(\"Generated Question:\")\n",
    "        print(answer_text)\n",
    "\n",
    "        # Show one source doc if present\n",
    "        if docs:\n",
    "            print(\"\\nüìö Based on content from:\")\n",
    "            doc = docs[0]\n",
    "            src = (doc.metadata.get(\"source\") or \"\").split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "            preview = (doc.page_content[:100] + \"...\") if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"   ‚Ä¢ {src}: {preview}\")\n",
    "        else:\n",
    "            print(\"\\nüìö No context docs returned.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating question: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef49062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 901\n",
      "Retrieved 3 chunks for 'linear equations':\n",
      "1. linearalgebra.pdf: 70 Systems of Linear Equations\n",
      "70...\n",
      "2. linearalgebra.pdf: G\n",
      "Movie Scripts\n",
      "G.1 What is Linear Algebra?\n",
      "Hint for Review Problem 5\n",
      "Looking at the problem stateme...\n"
     ]
    }
   ],
   "source": [
    "# Quick chunk check\n",
    "print(f\"Total chunks: {len(split_docs)}\")\n",
    "\n",
    "# Test one query\n",
    "docs = retriever.invoke(\"linear equations\")\n",
    "print(f\"Retrieved {len(docs)} chunks for 'linear equations':\")\n",
    "\n",
    "for i, doc in enumerate(docs[:2]):\n",
    "    source = doc.metadata.get('source', '').split('\\\\')[-1]\n",
    "    print(f\"{i+1}. {source}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96ddff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Quality Assessment\n",
      "========================================\n",
      "Query: Generate a simple linear equation for 8th grade\n",
      "Generated Answer: Question: Tom has 5 apples and 2 oranges in a basket. If oranges always have twice as much sugar as apples and each apple has 3 grams of sugar, how many grams of sugar does the basket contain?\n",
      "\n",
      "A) 5x ...\n",
      "Retrieved 3 source documents\n",
      "\n",
      "Source Document Relevance Check:\n",
      "1. linearalgebra.pdf: 70 Systems of Linear Equations\n",
      "70\n",
      "2. linearalgebra.pdf: G\n",
      "Movie Scripts\n",
      "G.1 What is Linear Algebra?\n",
      "Hint for Review Problem 5\n",
      "Looking at the problem statement we find some important information, first\n",
      "that ...\n",
      "\n",
      "Manual Review Points:\n",
      "‚Ä¢ Does the generated question match the 8th grade level?\n",
      "‚Ä¢ Are the retrieved documents relevant to linear equations?\n",
      "‚Ä¢ Is the answer format appropriate for the context?\n",
      "‚Ä¢ Are the mathematical concepts accurate?\n"
     ]
    }
   ],
   "source": [
    "# Simple RAG Quality Check (Manual Evaluation)\n",
    "\n",
    "print(\"RAG Quality Assessment\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_query = \"Generate a simple linear equation for 8th grade\"\n",
    "\n",
    "# v0.3+ chains are Runnables -> use .invoke with {\"input\": ...}\n",
    "out = qa_chain.invoke({\"input\": test_query})\n",
    "\n",
    "# Depending on minor versions, the text can be under \"answer\" or \"output_text\"\n",
    "answer_text = out.get(\"answer\") or out.get(\"output_text\") or str(out)\n",
    "docs = out.get(\"context\", [])   # retrieved documents are usually returned under \"context\"\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Generated Answer: {answer_text[:200]}...\")\n",
    "print(f\"Retrieved {len(docs)} source documents\")\n",
    "\n",
    "print(\"\\nSource Document Relevance Check:\")\n",
    "for i, doc in enumerate(docs[:2], 1):\n",
    "    source = (doc.metadata.get('source') or '').split('\\\\')[-1].split('/')[-1]\n",
    "    preview = (doc.page_content[:150] + \"...\") if len(doc.page_content) > 150 else doc.page_content\n",
    "    print(f\"{i}. {source}: {preview}\")\n",
    "\n",
    "print(\"\\nManual Review Points:\")\n",
    "print(\"‚Ä¢ Does the generated question match the 8th grade level?\")\n",
    "print(\"‚Ä¢ Are the retrieved documents relevant to linear equations?\")\n",
    "print(\"‚Ä¢ Is the answer format appropriate for the context?\")\n",
    "print(\"‚Ä¢ Are the mathematical concepts accurate?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85d074",
   "metadata": {},
   "source": [
    "## 7. Stage¬†6 ‚Äì Evaluation and Refinement\n",
    "\n",
    "Automatically generated questions should not be used blindly; users need to review and refine them.  Research in retrieval‚Äëaugmented item generation has shown that methods like key‚Äëpoint extraction and careful prompting can improve vital coverage, grammar and readability.  Some practical evaluation strategies include:\n",
    "\n",
    "- **Content alignment** ‚Äì verify that each generated item accurately assesses the intended concept and at the appropriate cognitive level (e.g., recall, application, analysis).\n",
    "- **Correctness and clarity** ‚Äì check that the question is unambiguous and that the answer provided is correct.\n",
    "- **Difficulty and distractor quality** ‚Äì adjust the difficulty of multiple‚Äëchoice questions and ensure distractors (incorrect options) are plausible but clearly wrong.\n",
    "\n",
    "For automation, **RAGAS** (Retrieval-Augmented Generation Assessment Suite) offers useful metrics:  \n",
    "- **Context relevance** ‚Äì retrieved passages match the query.  \n",
    "- **Faithfulness** ‚Äì generation stays true to the context.  \n",
    "- **Answer correctness** ‚Äì answer is supported by evidence.  \n",
    "\n",
    "Combining quick RAGAS diagnostics with **SME feedback** creates an efficient refinement loop, leading to higher-quality, trustworthy items.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b730b",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "By following these stages, collecting and splitting your knowledge base, embedding it, indexing it in a vector store, retrieving relevant passages, generating with context, and evaluating the results, you can build a retrieval‚Äëaugmented item generator tailored to your domain.  RAG‚Äôs strength lies in anchoring generative models to external knowledge, thereby producing responses that are both relevant and factual.  The LangChain framework provides convenient abstractions for each stage, and open‚Äësource models make it accessible to everyone without proprietary licenses.  Adapt the code provided to your own knowledge base and continue experimenting with different models and prompts to achieve the best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aigenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
