{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d17c43f",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) for Item Generation\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Overview\n",
    "\n",
    "Welcome to this hands-on workshop on using **Retrieval-Augmented Generation (RAG)** for item generation. This session will provide you with both theoretical understanding and practical implementation skills to build an AI-powered item generation tool that is grounded in authoritative content.\n",
    "\n",
    "### **What You'll Learn:**\n",
    "- The theoretical foundation of RAG and its applications in educational assessment\n",
    "- How to build a complete RAG pipeline using open-source tools\n",
    "- Best practices for generating high-quality, curriculum-aligned assessment items\n",
    "- Quality assurance and evaluation frameworks for AI-generated content\n",
    "- Ethical considerations and limitations in automated item generation\n",
    "\n",
    "### **Why RAG for Item Development?**\n",
    "Traditional AI language models can \"hallucinate\" or generate content that sounds plausible but isn't grounded in verified educational standards. RAG solves this by:\n",
    "\n",
    "**Retrieving** relevant content from authoritative sources (curriculum standards, textbooks, learning objectives)  \n",
    "**Augmenting** the language model with this context  \n",
    "**Generating** assessment items that are both creative and factually accurate\n",
    "\n",
    "Think of it as giving the AI a \"reference library\" before it writes your exam questions thereby ensuring every generated item is anchored to verified content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec31ce",
   "metadata": {},
   "source": [
    "## 1. Pipeline Overview\n",
    "\n",
    "A typical RAG pipeline consists of several stages.  Each stage plays a distinct role in making sure that the final generated item reflects accurate curriculum content and is easy for educators to trust:\n",
    "\n",
    "1. **Data ingestion and document preparation** – gather curricular materials and convert them into a uniform format that the computer can process, e.g, PDFs.\n",
    "\n",
    "2. **Splitting the documents (“chunking”)** – long texts are divided into smaller segments or *chunks*.  This is like breaking a textbook chapter into paragraphs so that the system can “digest” them.  Chunking is essential because language models can only process a limited amount of text at once; breaking the text into manageable pieces ensures that important details are not lost.\n",
    "\n",
    "3. **Embedding the documents** – each chunk is transformed into a numerical vector that captures its meaning.  Embeddings are like fingerprints for text: they allow the computer to measure which passages are most similar to a given query.\n",
    "\n",
    "4. **Vector store indexing** – all of these vectors are stored in a database designed to support similarity search.  You can think of it as a special index that lets you quickly find passages related to a topic.\n",
    "\n",
    "5. **Query and retrieval** – when you have a question or item to generate, your query is also embedded and compared against the database to retrieve the most relevant chunks.\n",
    "\n",
    "6. **Generation with context** – the retrieved text is combined with a large language model (LLM) to produce the assessment item.  Conditioning the model on actual curriculum content helps reduce hallucinations and ensures fidelity to the source material.\n",
    "\n",
    "7. **Evaluation and refinement** – finally, review and refine the generated items.  Research shows that techniques like key‑point extraction and careful prompting can improve coverage, grammar, and readability of items.\n",
    "\n",
    "In the following sections, we explore each stage in detail, with code examples using open‑source models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aab3a6",
   "metadata": {},
   "source": [
    "## RAG Pipeline Visualization\n",
    "\n",
    "The following diagram illustrates the complete RAG pipeline workflow:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"RAG_pipeline.png\" alt=\"RAG Pipeline Diagram\" width=\"700\" style=\"border: 1px solid #ddd; border-radius: 8px; padding: 10px;\">\n",
    "</div>\n",
    "\n",
    "*Figure 1: RAG Pipeline - From document ingestion to final item generation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f38bc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bb50371",
   "metadata": {},
   "source": [
    "## 2. Stage 1 – Data Ingestion and Preparation\n",
    "\n",
    "The first step is to collect the domain‑specific materials, which can be textbook chapters, lecture notes, curriculum standards, or any other documents that contain the knowledge your assessment should be based on.  These materials form the **knowledge base** that the RAG pipeline will consult.\n",
    "\n",
    "Once collected, we need to convert them into a format that LangChain can process.  This involves reading files from disk and, importantly, **splitting** long documents into smaller pieces.  Splitting (also called *chunking*) is necessary because both embedding models and generative models have a maximum context length.  By dividing a document into chunks, we ensure that each piece captures a coherent passage (for example, a paragraph or half‑page) and can be processed independently.  Later, when we search the knowledge base, we will be comparing these chunks for relevance.\n",
    "\n",
    "Below is an example using LangChain’s `DirectoryLoader` and `RecursiveCharacterTextSplitter` to load `.txt` files from a directory and split them into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93819561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 468 documents and split into 901 chunks.\n",
      "\n",
      "Sample chunk from data\\linearalgebra.pdf:\n",
      "Linear Algebra\n",
      "David Cherney, Tom Denton,\n",
      "Rohit Thomas and Andrew Waldron...\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Path to your directory containing curricular PDF files.\n",
    "# Place PDF files in the data folder; each will be read as a separate document.\n",
    "DATA_DIR = \"./data\"  # Points to the data directory\n",
    "\n",
    "# Use DirectoryLoader to read PDF files into LangChain Document objects.\n",
    "# Each PDF becomes a Document with metadata about its source.\n",
    "directory_loader = DirectoryLoader(DATA_DIR, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = directory_loader.load()\n",
    "\n",
    "# Split long documents into smaller chunks.  The chunk_size and chunk_overlap parameters\n",
    "# control the length of each chunk and how much neighbouring chunks overlap.  The overlap\n",
    "# helps preserve context across boundaries.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # number of characters per chunk\n",
    "    chunk_overlap=200 # overlapping characters between chunks to preserve context\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "print(f\"Loaded {len(documents)} documents and split into {len(split_docs)} chunks.\")\n",
    "\n",
    "# Optional: Show a sample of what was loaded\n",
    "if split_docs:\n",
    "    print(f\"\\nSample chunk from {split_docs[0].metadata.get('source', 'unknown')}:\")\n",
    "    print(split_docs[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ab847",
   "metadata": {},
   "source": [
    "## 3. Stage 2 – Embedding Documents\n",
    "\n",
    "After splitting the documents, we translate each chunk into a numeric representation called an *embedding*.  An embedding model is a type of neural network that maps a sentence or paragraph to a high‑dimensional vector such that semantically similar texts are close together in this space.  This translation step is crucial because it allows the computer to compare your query against thousands of document chunks quickly using simple mathematical operations.\n",
    "\n",
    "LangChain wraps many open‑source embedding models from Hugging Face.  For instance, `sentence-transformers/all-MiniLM-L6-v2` is a lightweight model that produces 384‑dimensional vectors well‑suited for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef80aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\makin\\Desktop\\AIME_Training_Doc\\AIME_Con_AI_Itemgeneration\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\makin\\Desktop\\AIME_Training_Doc\\AIME_Con_AI_Itemgeneration\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\makin\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\makin\\Desktop\\AIME_Training_Doc\\AIME_Con_AI_Itemgeneration\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\makin\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed 901 embeddings using HuggingFace's all-MiniLM-L6-v2 model.\n",
      "Each embedding has 384 dimensions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Instantiate a HuggingFace embedding model. This model converts each chunk of text into\n",
    "# a high-dimensional vector (384 dimensions) that captures its semantic meaning.\n",
    "# The all-MiniLM-L6-v2 model is small, efficient, and runs locally - perfect for learning!\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use CPU for compatibility\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Normalize for better similarity search\n",
    ")\n",
    "\n",
    "# Compute embeddings for each split document. In a full application you would typically\n",
    "# pass the embedding model directly to the vector store without this intermediate step,\n",
    "# but computing them here demonstrates that each chunk is mapped to a numeric vector.\n",
    "embeddings = embedding_model.embed_documents([doc.page_content for doc in split_docs])\n",
    "print(f\"Computed {len(embeddings)} embeddings using HuggingFace's all-MiniLM-L6-v2 model.\")\n",
    "print(f\"Each embedding has {len(embeddings[0]) if embeddings else 0} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb42a64",
   "metadata": {},
   "source": [
    "## 4. Stage 3 – Building a Vector Store\n",
    "\n",
    "Once you have embeddings for all your document chunks, you need a way to organise and search them. A **vector store** is like a library catalogue for embeddings: it indexes each vector so that given a new query vector, it can quickly find the most similar ones. \n",
    "\n",
    "We'll use **ChromaDB**, an excellent open-source vector database that's easy to install and perfect for learning. ChromaDB automatically handles persistence, requires no complex setup, and provides fast similarity search. It stores each chunk's embedding together with its original text, making retrieval efficient and reliable.\n",
    "\n",
    "**Why ChromaDB?**\n",
    "- Simple installation: `pip install chromadb`\n",
    "- Automatic persistence to disk\n",
    "- No complex dependencies \n",
    "- Fast and reliable similarity search\n",
    "- Perfect for development and production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f61c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Creating ChromaDB vector store...\n",
      "✅ ChromaDB vector store created successfully!\n",
      "📁 Stored in: ./chroma_db\n",
      "📊 Indexed 901 document chunks\n",
      "💾 Vector store persisted to disk for future use\n",
      "\n",
      "🎯 ChromaDB Benefits:\n",
      "  • Easy installation (pip install chromadb)\n",
      "  • Automatic persistence to disk\n",
      "  • Fast similarity search\n",
      "  • No additional dependencies like FAISS\n",
      "  • Perfect for learning and development\n"
     ]
    }
   ],
   "source": [
    "# Using ChromaDB - A simple, fast vector database that's easy to install\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "# Create a unique directory for this session's vector store\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Build a ChromaDB vector store directly from your split documents and embedding model\n",
    "print(\"🚀 Creating ChromaDB vector store...\")\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(\"✅ ChromaDB vector store created successfully!\")\n",
    "print(f\"📁 Stored in: {persist_directory}\")\n",
    "print(f\"📊 Indexed {len(split_docs)} document chunks\")\n",
    "print(f\"💾 Vector store persisted to disk for future use\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde475f9",
   "metadata": {},
   "source": [
    "## 5. Stage 4 – Query and Retrieval\n",
    "\n",
    "To generate a new question, the user starts by formulating a **query**, which is a short prompt of the items to generate.  For example, “Provide 10 questions on linear algebra.”  This query is embedded using the same embedding model as before.  The vector store then finds the chunks whose embeddings are most similar to the query vector.  Retrieving these top‑`k` chunks is similar to using a search engine: the model is effectively saying “these passages from the curriculum best answer your question.”  We will later feed these passages to the generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "920c5ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading ChromaDB vector store from disk...\n",
      "✅ ChromaDB vector store loaded successfully!\n",
      "🔍 Searching for: 'Linear algebra concepts suitable for high school students'\n",
      "\n",
      "📄 Retrieved 3 relevant document chunks:\n",
      "======================================================================\n",
      "\n",
      "📋 Chunk 1 (Source: data\\linearalgebra.pdf):\n",
      "--------------------------------------------------\n",
      "1\n",
      "What is Linear Algebra?\n",
      "Many diﬃcult problems can be handled easily once relevant information is\n",
      "organized in a certain way. This text aims to teach you how to organize in-\n",
      "formation in cases where certain mathematical structures are present. Linear\n",
      "algebra is, in general, the study of those struc...\n",
      "--------------------------------------------------\n",
      "\n",
      "📋 Chunk 2 (Source: data\\linearalgebra.pdf):\n",
      "--------------------------------------------------\n",
      "12 What is Linear Algebra?\n",
      "This example is a hint at a much bigger idea central to the text; our choice of\n",
      "order is an example of choosing a basis3.\n",
      "The main lesson of an introductory linear algebra course is this: you\n",
      "have considerable freedom in how you organize information about certain\n",
      "functions...\n",
      "--------------------------------------------------\n",
      "\n",
      "📋 Chunk 3 (Source: data\\linearalgebra.pdf):\n",
      "--------------------------------------------------\n",
      "Linear Algebra\n",
      "David Cherney, Tom Denton,\n",
      "Rohit Thomas and Andrew Waldron\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the ChromaDB vector store \n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Load the existing ChromaDB vector store from disk\n",
    "print(\"🔄 Loading ChromaDB vector store from disk...\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "print(\"✅ ChromaDB vector store loaded successfully!\")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\": 3}  # Return top 3 most similar chunks\n",
    ")\n",
    "\n",
    "# Example query: specify the concept you want to generate an item about\n",
    "query = \"Linear algebra concepts suitable for high school students\"\n",
    "print(f\"🔍 Searching for: '{query}'\")\n",
    "\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"\\n📄 Retrieved {len(retrieved_docs)} relevant document chunks:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    source = doc.metadata.get('source', 'unknown').split('/')[-1]  # Get filename only\n",
    "    print(f\"\\n📋 Chunk {i} (Source: {source}):\")\n",
    "    print(\"-\" * 50)\n",
    "    # Show first 300 characters for readability\n",
    "    content = doc.page_content.strip()\n",
    "    display_content = content[:300] + \"...\" if len(content) > 300 else content\n",
    "    print(display_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda139a",
   "metadata": {},
   "source": [
    "## 6. Stage 5 – Generation with Context\n",
    "\n",
    "The heart of the RAG pipeline is the generation step.  Here we take the relevant passages retrieved in the previous stage and combine them with the query to form a prompt for a generative language model.  The model then produces a new assessment item (question and answer) that draws explicitly from the provided context.  This step reduces hallucination because the model is “reminded” of the facts that should guide its answer.\n",
    "\n",
    "We use Groq-hosted open-source language models such as LLaMA 3 or Mixtral, which are fast, optimized, and freely accessible via the Groq API. In practice, you might choose a larger or more specialized model, but the overall code pattern remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ae1969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GROQ_API_KEY loaded successfully from .env file\n",
      "Generating 8th Grade Linear Algebra Questions\n",
      "============================================================\n",
      "\n",
      "🔢 Question Set 1:\n",
      "Query: Generate an 8th grade question about solving simple linear equations with one variable\n",
      "--------------------------------------------------\n",
      "Generated Question:\n",
      "Question: Tom has 5 apples, and his friend has some apples. If Tom's friend has twice as many apples as Tom, how many apples does his friend have? \n",
      "\n",
      "A) 2x + 5 = 10\n",
      "B) 2x - 5 = 10\n",
      "C) 2x + 5 = 15\n",
      "D) 2x - 5 = 15\n",
      "\n",
      "Correct Answer: C) 2x + 5 = 15\n",
      "\n",
      "Explanation: Let's say the number of apples Tom's friend has is x. Since he has twice as many apples as Tom, we can write the equation as x = 2(5). This simplifies to x = 10. However, we are given that Tom's friend has some apples (x), and that this number is related to Tom's apples (5). To represent this relationship, we can write the equation x = 2(5) + 0 or x = 2(5) + (x - 5), but we can also write it as x = 2(5) + 5. This is the same as 2x + 5 = 10 + 5, which simplifies to 2x + 5 = 15.\n",
      "\n",
      "📚 Based on content from:\n",
      "   • linearalgebra.pdf: G\n",
      "Movie Scripts\n",
      "G.1 What is Linear Algebra?\n",
      "Hint for Review Problem 5\n",
      "Looking at the problem stateme...\n",
      "\n",
      "============================================================\n",
      "\n",
      "🔢 Question Set 2:\n",
      "Query: Create a basic algebra problem suitable for middle school students involving solving for x\n",
      "--------------------------------------------------\n",
      "Generated Question:\n",
      "Question: Tom has 5 apples, and his friend has some apples. If Tom's friend has twice as many apples as Tom, how many apples does his friend have? \n",
      "\n",
      "A) 2x + 5 = 10\n",
      "B) 2x - 5 = 10\n",
      "C) 2x + 5 = 15\n",
      "D) 2x - 5 = 15\n",
      "\n",
      "Correct Answer: C) 2x + 5 = 15\n",
      "\n",
      "Explanation: Let's say the number of apples Tom's friend has is x. Since he has twice as many apples as Tom, we can write the equation as x = 2(5). This simplifies to x = 10. However, we are given that Tom's friend has some apples (x), and that this number is related to Tom's apples (5). To represent this relationship, we can write the equation x = 2(5) + 0 or x = 2(5) + (x - 5), but we can also write it as x = 2(5) + 5. This is the same as 2x + 5 = 10 + 5, which simplifies to 2x + 5 = 15.\n",
      "\n",
      "📚 Based on content from:\n",
      "   • linearalgebra.pdf: G\n",
      "Movie Scripts\n",
      "G.1 What is Linear Algebra?\n",
      "Hint for Review Problem 5\n",
      "Looking at the problem stateme...\n",
      "\n",
      "============================================================\n",
      "\n",
      "🔢 Question Set 2:\n",
      "Query: Create a basic algebra problem suitable for middle school students involving solving for x\n",
      "--------------------------------------------------\n",
      "Generated Question:\n",
      "Question: Tom has 2x + 5 apples. He gives 3 of them to his friend. How many apples does Tom have left?\n",
      "\n",
      "A) 2x - 3\n",
      "B) 2x + 2\n",
      "C) 2x + 8\n",
      "D) x - 3\n",
      "\n",
      "Correct Answer: A) 2x - 3\n",
      "\n",
      "Explanation: To find the number of apples Tom has left, we need to subtract the number of apples he gave to his friend from the total number of apples he had.\n",
      "\n",
      "Original number of apples: 2x + 5\n",
      "Number of apples given away: 3\n",
      "\n",
      "New number of apples: 2x + 5 - 3\n",
      "\n",
      "Combine like terms: 2x - 3\n",
      "\n",
      "So, Tom has 2x - 3 apples left.\n",
      "\n",
      "📚 Based on content from:\n",
      "   • linearalgebra.pdf: 72 The Simplex Method\n",
      "Finally Pablo knows that oranges have twice as much sugar as apples and that a...\n",
      "\n",
      "============================================================\n",
      "\n",
      "🔢 Question Set 3:\n",
      "Query: Generate a linear equation problem that 8th graders can solve in 2-3 steps\n",
      "--------------------------------------------------\n",
      "Generated Question:\n",
      "Question: Tom has 2x + 5 apples. He gives 3 of them to his friend. How many apples does Tom have left?\n",
      "\n",
      "A) 2x - 3\n",
      "B) 2x + 2\n",
      "C) 2x + 8\n",
      "D) x - 3\n",
      "\n",
      "Correct Answer: A) 2x - 3\n",
      "\n",
      "Explanation: To find the number of apples Tom has left, we need to subtract the number of apples he gave to his friend from the total number of apples he had.\n",
      "\n",
      "Original number of apples: 2x + 5\n",
      "Number of apples given away: 3\n",
      "\n",
      "New number of apples: 2x + 5 - 3\n",
      "\n",
      "Combine like terms: 2x - 3\n",
      "\n",
      "So, Tom has 2x - 3 apples left.\n",
      "\n",
      "📚 Based on content from:\n",
      "   • linearalgebra.pdf: 72 The Simplex Method\n",
      "Finally Pablo knows that oranges have twice as much sugar as apples and that a...\n",
      "\n",
      "============================================================\n",
      "\n",
      "🔢 Question Set 3:\n",
      "Query: Generate a linear equation problem that 8th graders can solve in 2-3 steps\n",
      "--------------------------------------------------\n",
      "Generated Question:\n",
      "Question: Tom wants to make a fruit salad with at least 8 apples and 3 oranges. The total number of fruits he can have is no more than 20. If each apple contributes 1 point and each orange contributes 2 points, what is the maximum number of points Tom can have?\n",
      "\n",
      "A) x + 2y = 15\n",
      "B) x + 2y = 25\n",
      "C) x + 2y = 10\n",
      "D) x + 2y = 5\n",
      "\n",
      "Correct Answer: B) x + 2y = 25\n",
      "\n",
      "Explanation: \n",
      "Step 1: Tom wants to make a fruit salad with at least 8 apples and 3 oranges. This can be represented by the inequality x ≥ 8 and y ≥ 3.\n",
      "Step 2: The total number of fruits Tom can have is no more than 20. This can be represented by the inequality x + y ≤ 20.\n",
      "Step 3: To find the maximum number of points Tom can have, we need to find the maximum value of the expression 1x + 2y. Since x + y ≤ 20, the maximum value of x + 2y will occur when x + y = 20. Therefore, the equation representing the maximum number of points is x + 2y = 20 (not 15, 10 or 5).\n",
      "\n",
      "📚 Based on content from:\n",
      "   • linearalgebra.pdf: 72 The Simplex Method\n",
      "Finally Pablo knows that oranges have twice as much sugar as apples and that a...\n",
      "\n",
      "============================================================\n",
      "Generated Question:\n",
      "Question: Tom wants to make a fruit salad with at least 8 apples and 3 oranges. The total number of fruits he can have is no more than 20. If each apple contributes 1 point and each orange contributes 2 points, what is the maximum number of points Tom can have?\n",
      "\n",
      "A) x + 2y = 15\n",
      "B) x + 2y = 25\n",
      "C) x + 2y = 10\n",
      "D) x + 2y = 5\n",
      "\n",
      "Correct Answer: B) x + 2y = 25\n",
      "\n",
      "Explanation: \n",
      "Step 1: Tom wants to make a fruit salad with at least 8 apples and 3 oranges. This can be represented by the inequality x ≥ 8 and y ≥ 3.\n",
      "Step 2: The total number of fruits Tom can have is no more than 20. This can be represented by the inequality x + y ≤ 20.\n",
      "Step 3: To find the maximum number of points Tom can have, we need to find the maximum value of the expression 1x + 2y. Since x + y ≤ 20, the maximum value of x + 2y will occur when x + y = 20. Therefore, the equation representing the maximum number of points is x + 2y = 20 (not 15, 10 or 5).\n",
      "\n",
      "📚 Based on content from:\n",
      "   • linearalgebra.pdf: 72 The Simplex Method\n",
      "Finally Pablo knows that oranges have twice as much sugar as apples and that a...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Install the Groq LangChain integration if needed:\n",
    "# !pip install -U langchain_groq\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# 1) Configure the Groq model (uses open-source OSS models hosted by Groq)\n",
    "# The API key is now loaded from the .env file\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    print(\"❌ GROQ_API_KEY not found in .env file\")\n",
    "    print(\"Please check that your .env file contains: GROQ_API_KEY='your_key_here'\")\n",
    "else:\n",
    "    print(\"✅ GROQ_API_KEY loaded successfully from .env file\")\n",
    "\n",
    "# Pick an OSS model served by Groq. Current supported models include:\n",
    "# - \"llama-3.1-8b-instant\" (recommended)\n",
    "# - \"llama-3.1-70b-versatile\"\n",
    "# - \"mixtral-8x7b-32768\"\n",
    "# - \"gemma2-9b-it\"\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  # Updated to current supported model\n",
    "    temperature=0.6,     # lower = more deterministic\n",
    "    max_tokens=600\n",
    ")\n",
    "\n",
    "# 2) Prompt for 8th Grade Linear Algebra\n",
    "MATH_ITEM_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert 8th grade mathematics assessment writer specializing in linear algebra.\n",
    "Use ONLY the provided context to create ONE multiple-choice question suitable for 8th grade students.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Requirements for 8th Grade Linear Algebra:\n",
    "- Focus on basic linear equations (like ax + b = c or x + a = b)\n",
    "- Use simple integer solutions (avoid fractions when possible)\n",
    "- Create clear, direct question stems\n",
    "- EXACTLY 4 options labeled A–D with ONE correct answer\n",
    "- Make distractors based on common student errors:\n",
    "  * Wrong operation (adding instead of subtracting)\n",
    "  * Wrong direction (subtracting from wrong side)\n",
    "  * Arithmetic errors\n",
    "  * Not performing the operation\n",
    "- Provide step-by-step explanation\n",
    "- Use variables like x, y, m, n (single letters)\n",
    "- Keep numbers simple (typically 1-50)\n",
    "\n",
    "Follow this exact format:\n",
    "Question: [Clear problem statement]\n",
    "A) [Correct answer]\n",
    "B) [Common error - wrong operation]\n",
    "C) [Common error - arithmetic mistake]  \n",
    "D) [Common error - incomplete solution]\n",
    "Correct Answer: [Letter]\n",
    "Explanation: [Step-by-step solution showing the correct mathematical process]\n",
    "\n",
    "Examples of appropriate 8th grade topics from context:\n",
    "- Solving one-step equations: x + 5 = 12\n",
    "- Solving two-step equations: 2x + 3 = 11\n",
    "- Basic substitution problems\n",
    "- Simple linear relationships\n",
    "\n",
    "User goal: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 3) Build a RetrievalQA chain for 8th grade math\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,               # Uses ChromaDB retriever from Stage 4\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": MATH_ITEM_PROMPT}\n",
    ")\n",
    "\n",
    "# 4) Generate 8th grade linear algebra questions\n",
    "print(\"Generating 8th Grade Linear Algebra Questions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different types of 8th grade linear algebra problems\n",
    "grade_8_queries = [\n",
    "    \"Generate an 8th grade question about solving simple linear equations with one variable\",\n",
    "    \"Create a basic algebra problem suitable for middle school students involving solving for x\",\n",
    "    \"Generate a linear equation problem that 8th graders can solve in 2-3 steps\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(grade_8_queries, 1):\n",
    "    print(f\"\\n🔢 Question Set {i}:\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        response = qa_chain(query)\n",
    "        print(\"Generated Question:\")\n",
    "        print(response[\"result\"])\n",
    "        \n",
    "        # Show source context (optional)\n",
    "        print(f\"\\n📚 Based on content from:\")\n",
    "        for j, doc in enumerate(response[\"source_documents\"][:1], 1):  # Show only first source\n",
    "            source = doc.metadata.get('source', 'unknown')\n",
    "            if '/' in source or '\\\\' in source:\n",
    "                source = source.split('\\\\')[-1].split('/')[-1]\n",
    "            print(f\"   • {source}: {doc.page_content[:100]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating question: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef49062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 901\n",
      "Retrieved 3 chunks for 'linear equations':\n",
      "1. linearalgebra.pdf: 70 Systems of Linear Equations\n",
      "70...\n",
      "2. linearalgebra.pdf: G\n",
      "Movie Scripts\n",
      "G.1 What is Linear Algebra?\n",
      "Hint for Review Problem 5\n",
      "Looking at the problem stateme...\n"
     ]
    }
   ],
   "source": [
    "# Quick chunk check\n",
    "print(f\"Total chunks: {len(split_docs)}\")\n",
    "\n",
    "# Test one query\n",
    "docs = retriever.get_relevant_documents(\"linear equations\")\n",
    "print(f\"Retrieved {len(docs)} chunks for 'linear equations':\")\n",
    "\n",
    "for i, doc in enumerate(docs[:2]):\n",
    "    source = doc.metadata.get('source', '').split('\\\\')[-1]\n",
    "    print(f\"{i+1}. {source}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96ddff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 RAG Quality Assessment\n",
      "========================================\n",
      "Query: Generate a simple linear equation for 8th grade\n",
      "Generated Answer: Question: \n",
      "Tom has 3 bags of apples and 2 bags of oranges. If each bag of oranges has twice as many oranges as each bag of apples, how many oranges and apples are there in total?\n",
      "\n",
      "A) 3x + 2x = 10\n",
      "B) 3...\n",
      "Retrieved 3 source documents\n",
      "\n",
      "📚 Source Document Relevance Check:\n",
      "1. linearalgebra.pdf: 70 Systems of Linear Equations\n",
      "70...\n",
      "2. linearalgebra.pdf: G\n",
      "Movie Scripts\n",
      "G.1 What is Linear Algebra?\n",
      "Hint for Review Problem 5\n",
      "Looking at the problem statement we find some important information, first\n",
      "that ...\n",
      "\n",
      "✅ Manual Review Points:\n",
      "• Does the generated question match the 8th grade level?\n",
      "• Are the retrieved documents relevant to linear equations?\n",
      "• Is the answer format appropriate for the context?\n",
      "• Are the mathematical concepts accurate?\n",
      "Query: Generate a simple linear equation for 8th grade\n",
      "Generated Answer: Question: \n",
      "Tom has 3 bags of apples and 2 bags of oranges. If each bag of oranges has twice as many oranges as each bag of apples, how many oranges and apples are there in total?\n",
      "\n",
      "A) 3x + 2x = 10\n",
      "B) 3...\n",
      "Retrieved 3 source documents\n",
      "\n",
      "📚 Source Document Relevance Check:\n",
      "1. linearalgebra.pdf: 70 Systems of Linear Equations\n",
      "70...\n",
      "2. linearalgebra.pdf: G\n",
      "Movie Scripts\n",
      "G.1 What is Linear Algebra?\n",
      "Hint for Review Problem 5\n",
      "Looking at the problem statement we find some important information, first\n",
      "that ...\n",
      "\n",
      "✅ Manual Review Points:\n",
      "• Does the generated question match the 8th grade level?\n",
      "• Are the retrieved documents relevant to linear equations?\n",
      "• Is the answer format appropriate for the context?\n",
      "• Are the mathematical concepts accurate?\n"
     ]
    }
   ],
   "source": [
    "# Simple RAG Quality Check (Manual Evaluation)\n",
    "# Since RAGAS setup can be complex, let's do a simple manual evaluation\n",
    "\n",
    "print(\"📊 RAG Quality Assessment\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test question generation\n",
    "test_query = \"Generate a simple linear equation for 8th grade\"\n",
    "response = qa_chain(test_query)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Generated Answer: {response['result'][:200]}...\")\n",
    "print(f\"Retrieved {len(response['source_documents'])} source documents\")\n",
    "\n",
    "# Show relevance of retrieved documents\n",
    "print(\"\\n📚 Source Document Relevance Check:\")\n",
    "for i, doc in enumerate(response[\"source_documents\"][:2], 1):\n",
    "    source = doc.metadata.get('source', '').split('\\\\')[-1]\n",
    "    content = doc.page_content[:150]\n",
    "    print(f\"{i}. {source}: {content}...\")\n",
    "    \n",
    "print(\"\\n✅ Manual Review Points:\")\n",
    "print(\"• Does the generated question match the 8th grade level?\")\n",
    "print(\"• Are the retrieved documents relevant to linear equations?\") \n",
    "print(\"• Is the answer format appropriate for the context?\")\n",
    "print(\"• Are the mathematical concepts accurate?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85d074",
   "metadata": {},
   "source": [
    "## 7. Stage 6 – Evaluation and Refinement\n",
    "\n",
    "Automatically generated questions should not be used blindly; users need to review and refine them.  Research in retrieval‑augmented item generation has shown that methods like key‑point extraction and careful prompting can improve vital coverage, grammar and readability.  Some practical evaluation strategies include:\n",
    "\n",
    "- **Content alignment** – verify that each generated item accurately assesses the intended concept and at the appropriate cognitive level (e.g., recall, application, analysis).\n",
    "- **Correctness and clarity** – check that the question is unambiguous and that the answer provided is correct.\n",
    "- **Difficulty and distractor quality** – adjust the difficulty of multiple‑choice questions and ensure distractors (incorrect options) are plausible but clearly wrong.\n",
    "\n",
    "For automation, **RAGAS** (Retrieval-Augmented Generation Assessment Suite) offers useful metrics:  \n",
    "- **Context relevance** – retrieved passages match the query.  \n",
    "- **Faithfulness** – generation stays true to the context.  \n",
    "- **Answer correctness** – answer is supported by evidence.  \n",
    "\n",
    "Combining quick RAGAS diagnostics with **SME feedback** creates an efficient refinement loop, leading to higher-quality, trustworthy items.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b730b",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "By following these stages, collecting and splitting your knowledge base, embedding it, indexing it in a vector store, retrieving relevant passages, generating with context, and evaluating the results, you can build a retrieval‑augmented item generator tailored to your domain.  RAG’s strength lies in anchoring generative models to external knowledge, thereby producing responses that are both relevant and factual.  The LangChain framework provides convenient abstractions for each stage, and open‑source models make it accessible to everyone without proprietary licenses.  Adapt the code provided to your own knowledge base and continue experimenting with different models and prompts to achieve the best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
